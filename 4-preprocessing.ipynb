{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing jet-images\n",
    "Author: Javier Duarte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `NumPy` array files\n",
    "Here we load the `NumPy` array files. The files are available in a CERNBox: https://cernbox.cern.ch/index.php/s/hMHl6mYSqICOXKl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QCD120 1 files\n",
      "Getting 1 / 1 files\n",
      "/home/cms.woodson/machine-learning-hats/data/QCD_120to170/params0.npy_job0_file0.npy\n",
      "TT 22 files\n",
      "Getting 20 / 22 files\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job50_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job51_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job52_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job53_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job54_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job55_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job56_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job57_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job58_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job59_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job5_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job60_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job61_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job62_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job63_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job64_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job65_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job66_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job67_file0.npy\n",
      "/home/cms.woodson/machine-learning-hats/data/TT/params0.npy_job68_file0.npy\n",
      "QCD470 1 files\n",
      "Getting 1 / 1 files\n",
      "/home/cms.woodson/machine-learning-hats/data/QCD_470to600/params0.npy_job0_file0.npy\n",
      "QCD170 1 files\n",
      "Getting 1 / 1 files\n",
      "/home/cms.woodson/machine-learning-hats/data/QCD_170to300/params0.npy_job0_file0.npy\n",
      "QCD300 1 files\n",
      "Getting 1 / 1 files\n",
      "/home/cms.woodson/machine-learning-hats/data/QCD_300to470/params0.npy_job0_file0.npy\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "# get input numpy arrays\n",
    "inputs = {}\n",
    "inputs['TT'] = glob.glob('/home/cms.woodson/machine-learning-hats/data/TT/*job*.npy')\n",
    "inputs['QCD120'] = glob.glob('/home/cms.woodson/machine-learning-hats/data/QCD_120to170/*job*.npy')\n",
    "inputs['QCD170'] = glob.glob('/home/cms.woodson/machine-learning-hats/data/QCD_170to300/*job*.npy')\n",
    "inputs['QCD300'] = glob.glob('/home/cms.woodson/machine-learning-hats/data/QCD_300to470/*job*.npy')\n",
    "inputs['QCD470'] = glob.glob('/home/cms.woodson/machine-learning-hats/data/QCD_470to600/*job*.npy')\n",
    "\n",
    "import random\n",
    "fraction_file = 0.3\n",
    "max_file = 1\n",
    "\n",
    "list_params = {}\n",
    "params = {}\n",
    "for key, input_files in inputs.iteritems():\n",
    "    list_params[key] = []\n",
    "    print key,len(input_files),\"files\"\n",
    "    last_file = int(len(input_files)*fraction_file)+1 if fraction_file>0 else -1\n",
    "    last_file = min(max_file, len(input_files)) if max_file else last_file\n",
    "    if key=='TT': last_file*=20\n",
    "    print \"Getting\",last_file,\"/\",len(input_files),\"files\"\n",
    "    for in_file in input_files[:last_file]:\n",
    "        try:\n",
    "            print in_file\n",
    "            arr = np.load(in_file)\n",
    "            list_params[key].append(arr)\n",
    "        except ValueError:\n",
    "            print 'bad file: %s'%in_file\n",
    "        except IOError:\n",
    "            print 'bad io',in_file\n",
    "    params[key] = np.concatenate(list_params[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to `pandas` DataFrames\n",
    "Here, we convert to `pandas` DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of W jets: 1722\n",
      "number of QCD jets in bin QCD120: 37\n",
      "number of QCD jets in bin QCD170: 690\n",
      "number of QCD jets in bin QCD300: 1649\n",
      "number of QCD jets in bin QCD470: 929\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pt_min = 200\n",
    "pt_max = 500\n",
    "\n",
    "df_dict_jet = {}\n",
    "df_dict_jet['TT'] = pd.DataFrame(params['TT'],\n",
    "                                 columns=['run', 'lumi', 'event', 'met', 'sumet', 'rho', 'pthat', \n",
    "                                          'mcweight', 'njet_ak7', 'jet_pt_ak7', 'jet_eta_ak7', \n",
    "                                          'jet_phi_ak7', 'jet_E_ak7', 'jet_msd_ak7', 'jet_area_ak7', \n",
    "                                          'jet_jes_ak7', 'jet_tau21_ak7', 'jet_isW_ak7', 'jet_ncand_ak7',\n",
    "                                          'ak7pfcand_ijet'])\n",
    "df_dict_jet['TT'] = df_dict_jet['TT'].drop_duplicates()\n",
    "df_dict_jet['TT'] =  df_dict_jet['TT'][(df_dict_jet['TT'].jet_pt_ak7 > pt_min) & (df_dict_jet['TT'].jet_pt_ak7 < pt_max) &  (df_dict_jet['TT'].jet_isW_ak7==1)]\n",
    "\n",
    "df_dict_cand = {}\n",
    "df_dict_cand['TT'] = pd.DataFrame(params['TT'],\n",
    "                                  columns=['event', 'jet_pt_ak7', 'jet_isW_ak7', 'ak7pfcand_pt', 'ak7pfcand_eta', 'ak7pfcand_phi', 'ak7pfcand_id', 'ak7pfcand_charge', 'ak7pfcand_ijet'])\n",
    "df_dict_cand['TT'] =  df_dict_cand['TT'][(df_dict_cand['TT'].jet_pt_ak7 > pt_min) & (df_dict_cand['TT'].jet_pt_ak7 < pt_max) &  (df_dict_cand['TT'].jet_isW_ak7==1)]\n",
    "\n",
    "print 'number of W jets: %i'%len(df_dict_jet['TT'])\n",
    "\n",
    "for QCDbin in ['QCD120','QCD170','QCD300','QCD470']:\n",
    "    df_dict_jet[QCDbin] = pd.DataFrame(params[QCDbin],columns=['run', 'lumi', 'event', 'met', 'sumet', 'rho', 'pthat', \n",
    "                                                               'mcweight', 'njet_ak7', 'jet_pt_ak7', 'jet_eta_ak7', \n",
    "                                                               'jet_phi_ak7', 'jet_E_ak7', 'jet_msd_ak7', 'jet_area_ak7', \n",
    "                                                               'jet_jes_ak7', 'jet_tau21_ak7', 'jet_isW_ak7', 'jet_ncand_ak7',\n",
    "                                                               'ak7pfcand_ijet'])\n",
    "    df_dict_jet[QCDbin] = df_dict_jet[QCDbin].drop_duplicates()\n",
    "    df_dict_jet[QCDbin] =  df_dict_jet[QCDbin][(df_dict_jet[QCDbin].jet_pt_ak7 > pt_min) & (df_dict_jet[QCDbin].jet_pt_ak7 < 500) &  (df_dict_jet[QCDbin].jet_isW_ak7==0)]\n",
    "    # take every 20th jet just to make the training faster and have a sample roughly the size of W jets\n",
    "    df_dict_jet[QCDbin] = df_dict_jet[QCDbin].iloc[::20, :]\n",
    "    \n",
    "    df_dict_cand[QCDbin] = pd.DataFrame(params[QCDbin],columns=['event', 'jet_pt_ak7', 'jet_isW_ak7', 'ak7pfcand_pt', 'ak7pfcand_eta', 'ak7pfcand_phi', 'ak7pfcand_id', 'ak7pfcand_charge', 'ak7pfcand_ijet'])\n",
    "    df_dict_cand[QCDbin] =  df_dict_cand[QCDbin][(df_dict_cand[QCDbin].jet_pt_ak7 > 200) & (df_dict_cand[QCDbin].jet_pt_ak7 < 500) &  (df_dict_cand[QCDbin].jet_isW_ak7==0)]\n",
    "    \n",
    "    print 'number of QCD jets in bin %s: %i'%( QCDbin, len(df_dict_jet[QCDbin]))\n",
    "    \n",
    "df_dict_jet['QCD'] = pd.concat([df_dict_jet['QCD120'],df_dict_jet['QCD170'],df_dict_jet['QCD300'],df_dict_jet['QCD470']])\n",
    "df_dict_cand['QCD'] = pd.concat([df_dict_cand['QCD120'],df_dict_cand['QCD170'],df_dict_cand['QCD300'],df_dict_cand['QCD470']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for rotation and preprocessing jet images\n",
    "Here, we define functions for rotation and preprocessing jet images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# rotation + (possible) reflection needed later\n",
    "def rotate_and_reflect(x,y,w):\n",
    "    rot_x = []\n",
    "    rot_y = []\n",
    "    theta = 0\n",
    "    maxPt = -1\n",
    "    for ix, iy, iw in zip(x, y, w):\n",
    "        dv = np.matrix([[ix],[iy]])-np.matrix([[x.iloc[0]],[y.iloc[0]]])\n",
    "        dR = np.linalg.norm(dv)\n",
    "        thisPt = iw\n",
    "        if dR > 0.35 and thisPt > maxPt:\n",
    "            maxPt = thisPt\n",
    "            # rotation in eta-phi plane c.f  https://arxiv.org/abs/1407.5675 and https://arxiv.org/abs/1511.05190:\n",
    "            # theta = -np.arctan2(iy,ix)-np.radians(90)\n",
    "            # rotation by lorentz transformation c.f. https://arxiv.org/abs/1704.02124:\n",
    "            px = iw * np.cos(iy)\n",
    "            py = iw * np.sin(iy)\n",
    "            pz = iw * np.sinh(ix)\n",
    "            theta = np.arctan2(py,pz)+np.radians(90)\n",
    "            \n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.matrix('{} {}; {} {}'.format(c, -s, s, c))\n",
    "    for ix, iy, iw in zip(x, y, w):\n",
    "        # rotation in eta-phi plane:\n",
    "        #rot = R*np.matrix([[ix],[iy]])\n",
    "        #rix, riy = rot[0,0], rot[1,0]\n",
    "        # rotation by lorentz transformation\n",
    "        px = iw * np.cos(iy)\n",
    "        py = iw * np.sin(iy)\n",
    "        pz = iw * np.sinh(ix)\n",
    "        rot = R*np.matrix([[py],[pz]])\n",
    "        px1 = px\n",
    "        py1 = rot[0,0]\n",
    "        pz1 = rot[1,0]\n",
    "        iw1 = np.sqrt(px1*px1+py1*py1)\n",
    "        rix, riy = np.arcsinh(pz1/iw1), np.arcsin(py1/iw1)\n",
    "        rot_x.append(rix)\n",
    "        rot_y.append(riy)\n",
    "        \n",
    "    # now reflect if leftSum > rightSum\n",
    "    leftSum = 0\n",
    "    rightSum = 0\n",
    "    for ix, iy, iw in zip(x, y, w):\n",
    "        if ix > 0: \n",
    "            rightSum += iw\n",
    "        elif ix < 0:\n",
    "            leftSum += iw\n",
    "    if leftSum > rightSum:\n",
    "        ref_x = [-1.*rix for rix in rot_x]\n",
    "        ref_y = rot_y\n",
    "    else:\n",
    "        ref_x = rot_x\n",
    "        ref_y = rot_y\n",
    "    \n",
    "    return np.array(ref_x), np.array(ref_y)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import keras.backend as K\n",
    "\n",
    "nx = 30 # size of image in eta\n",
    "ny = 30 # size of image in phi\n",
    "xbins = np.linspace(-1.4,1.4,nx+1)\n",
    "ybins = np.linspace(-1.4,1.4,ny+1)\n",
    "\n",
    "def prepareImages(process, df_dict_cand, df_dict_jet):\n",
    "    list_x = []\n",
    "    list_y = []\n",
    "    list_w = []\n",
    "    if K.image_dim_ordering()=='tf':\n",
    "        jet_images = np.zeros((len(df_dict_jet[process]), nx, ny, 1))\n",
    "    else:        \n",
    "        jet_images = np.zeros((len(df_dict_jet[process]), 1, nx, ny))\n",
    "    \n",
    "    for i in range(0,len(df_dict_jet[process])):\n",
    "        # get the ith jet\n",
    "        df_dict_cand_i = df_dict_cand[process][(df_dict_cand[process]['ak7pfcand_ijet'] == df_dict_jet[process]['ak7pfcand_ijet'].iloc[i]) & (df_dict_cand[process]['event'] == df_dict_jet[process]['event'].iloc[i]) ]\n",
    "        # relative eta\n",
    "        x = df_dict_cand_i['ak7pfcand_eta']-df_dict_cand_i['ak7pfcand_eta'].iloc[0]\n",
    "        # relative phi\n",
    "        y = df_dict_cand_i['ak7pfcand_phi']-df_dict_cand_i['ak7pfcand_phi'].iloc[0]\n",
    "        weights = df_dict_cand_i['ak7pfcand_pt'] # pt of candidate is the weight\n",
    "        x,y = rotate_and_reflect(x,y,weights)\n",
    "        list_x.append(x)\n",
    "        list_y.append(y)\n",
    "        list_w.append(weights)\n",
    "        hist, xedges, yedges = np.histogram2d(x, y, weights=weights, bins=(xbins,ybins))\n",
    "        for ix in range(0,nx):\n",
    "            for iy in range(0,ny):\n",
    "                if K.image_dim_ordering()=='tf':\n",
    "                    jet_images[i,ix,iy,0] = hist[ix,iy]\n",
    "                else:\n",
    "                    jet_images[i,0,ix,iy] = hist[ix,iy]\n",
    "    return jet_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare jet images\n",
    "Here, we run the pre-defined functions to prepare the jet images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_images_dict = {}\n",
    "# 4D tensor (tensorflow backend)\n",
    "# 1st dim is jet index\n",
    "# 2nd dim is eta bin\n",
    "# 3rd dim is phi bin\n",
    "# 4th dim is pt value (or rgb layer, etc.)\n",
    "\n",
    "jet_images_dict['TT'] = prepareImages('TT', df_dict_cand, df_dict_jet)\n",
    "jet_images_dict['QCD'] = prepareImages('QCD', df_dict_cand, df_dict_jet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write jet images to `HDF5` file as `NumPy` arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QCD\n",
      "TT\n"
     ]
    }
   ],
   "source": [
    "df_particles = {}\n",
    "jet_particles = {}\n",
    "\n",
    "print('QCD')\n",
    "df_particles['QCD'] = df_dict_cand['QCD'][['ak7pfcand_pt', 'ak7pfcand_eta', 'ak7pfcand_phi', 'ak7pfcand_id', 'ak7pfcand_charge', 'event', 'ak7pfcand_ijet']]\n",
    "df_particles['QCD'].set_index(['event', 'ak7pfcand_ijet'], inplace=True)\n",
    "new_index = pd.Index(['_'.join([str(_) for _ in v]) for v in df_particles['QCD'].index.values])\n",
    "df_particles['QCD'].set_index(new_index, inplace=True)\n",
    "jet_particles['QCD'] = np.zeros((df_particles['QCD'].index.nunique(),100,len(df_particles['QCD'].columns)))\n",
    "grouped = df_particles['QCD'].groupby(level=0)\n",
    "for i, (j, g) in enumerate(grouped):\n",
    "    nmax = min(len(g),100)\n",
    "    jet_particles['QCD'][i,0:nmax,:] = g.values[0:nmax,:]\n",
    "    \n",
    "print('TT')\n",
    "df_particles['TT'] = df_dict_cand['TT'][['ak7pfcand_pt', 'ak7pfcand_eta', 'ak7pfcand_phi', 'ak7pfcand_id', 'ak7pfcand_charge', 'event', 'ak7pfcand_ijet']]\n",
    "df_particles['TT'].set_index(['event', 'ak7pfcand_ijet'], inplace=True)\n",
    "new_index = pd.Index(['_'.join([str(_) for _ in v]) for v in df_particles['TT'].index.values])\n",
    "df_particles['TT'].set_index(new_index, inplace=True)\n",
    "jet_particles['TT'] = np.zeros((df_particles['TT'].index.nunique(),100,len(df_particles['TT'].columns)))\n",
    "grouped = df_particles['TT'].groupby(level=0)\n",
    "for i, (j, g) in enumerate(grouped):\n",
    "    nmax = min(len(g),100)\n",
    "    jet_particles['TT'][i,0:nmax,:] = g.values[0:nmax,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File(\"data/jet_images.h5\", \"w\")\n",
    "h5f.create_dataset('QCD', data=jet_images_dict['QCD'], compression='lzf')\n",
    "h5f.create_dataset('TT', data=jet_images_dict['TT'], compression='lzf')\n",
    "h5f.create_dataset('QCD_particles', data = jet_particles['QCD'], compression='lzf')\n",
    "h5f.create_dataset('TT_particles', data = jet_particles['TT'], compression='lzf')\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-hats-2020",
   "language": "python",
   "name": "machine-learning-hats-2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
